@ARTICLE{Collins2003-yh,
  title    = "{Head-Driven} Statistical Models for Natural Language Parsing",
  author   = "Collins, Michael",
  journal  = "Comput. Linguist.",
  volume   =  29,
  number   =  4,
  pages    = "589--637",
  year     =  2003,
  keywords = "anthology;co-train"
}

@INPROCEEDINGS{Magerman1995-ga,
  title     = "Statistical {Decision-Tree} Models for Parsing",
  booktitle = "33rd Annual Meeting of the Association for Computational
               Linguistics",
  author    = "Magerman, David M",
  publisher = "Association for Computational Linguistics",
  pages     = "276--283",
  month     =  jun,
  year      =  1995,
  address   = "Cambridge, Massachusetts, USA",
  keywords  = "anthology;co-train"
}

@INPROCEEDINGS{Jin2019-so,
  title     = "Variance of Average Surprisal: A Better Predictor for Quality of
               Grammar from Unsupervised {PCFG} Induction",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  author    = "Jin, Lifeng and Schuler, William",
  publisher = "Association for Computational Linguistics",
  pages     = "2453--2463",
  month     =  jul,
  year      =  2019,
  address   = "Florence, Italy",
  keywords  = "anthology;co-train"
}

@article{liu2021self,
 author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 publisher = {IEEE},
 title = {Self-supervised learning: Generative or contrastive},
 year = {2021}
}

@article{chapelle2009semi,
 author = {Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
 journal = {IEEE Transactions on Neural Networks},
 number = {3},
 pages = {542--542},
 publisher = {IEEE},
 title = {Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]},
 volume = {20},
 year = {2009}
}

@article{yang2021survey,
 author = {Yang, Xiangli and Song, Zixing and King, Irwin and Xu, Zenglin},
 journal = {ArXiv preprint},
 title = {A survey on deep semi-supervised learning},
 url = {https://arxiv.org/abs/2103.00550},
 volume = {abs/2103.00550},
 year = {2021}
}

@article{zhou2018brief,
 author = {Zhou, Zhi-Hua},
 journal = {National science review},
 number = {1},
 pages = {44--53},
 publisher = {Oxford University Press},
 title = {A brief introduction to weakly supervised learning},
 volume = {5},
 year = {2018}
}

@article{scudder1965probability,
 author = {Scudder, Henry},
 journal = {IEEE Transactions on Information Theory},
 number = {3},
 pages = {363--371},
 publisher = {IEEE},
 title = {Probability of error of some adaptive pattern-recognition machines},
 volume = {11},
 year = {1965}
}

@inproceedings{lee2013pseudo,
 author = {Lee, Dong-Hyun and others},
 booktitle = {Workshop on challenges in representation learning, ICML},
 pages = {896},
 title = {Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
 volume = {3},
 year = {2013}
}

@inproceedings{dong2011ensemble,
 address = {Chiang Mai, Thailand},
 author = {Dong, Cailing  and
Sch{\"a}fer, Ulrich},
 booktitle = {Proceedings of 5th International Joint Conference on Natural Language Processing},
 pages = {623--631},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Ensemble-style Self-training on Citation Classification},
 url = {https://aclanthology.org/I11-1070},
 year = {2011}
}

@inproceedings{kahn2020self,
 author = {Jacob Kahn and
Ann Lee and
Awni Hannun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icassp/Kahn0H20.bib},
 booktitle = {2020 {IEEE} International Conference on Acoustics, Speech and Signal
Processing, {ICASSP} 2020, Barcelona, Spain, May 4-8, 2020},
 doi = {10.1109/ICASSP40776.2020.9054295},
 pages = {7084--7088},
 publisher = {{IEEE}},
 timestamp = {Thu, 23 Jul 2020 01:00:00 +0200},
 title = {Self-Training for End-to-End Speech Recognition},
 url = {https://doi.org/10.1109/ICASSP40776.2020.9054295},
 year = {2020}
}

@inproceedings{wang2019attributed,
 author = {Chun Wang and
Shirui Pan and
Ruiqi Hu and
Guodong Long and
Jing Jiang and
Chengqi Zhang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/ijcai/WangPHLJZ19.bib},
 booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
Artificial Intelligence, {IJCAI} 2019, Macao, China, August 10-16,
2019},
 doi = {10.24963/ijcai.2019/509},
 editor = {Sarit Kraus},
 pages = {3670--3676},
 publisher = {ijcai.org},
 timestamp = {Tue, 29 Dec 2020 00:00:00 +0100},
 title = {Attributed Graph Clustering: {A} Deep Attentional Embedding Approach},
 url = {https://doi.org/10.24963/ijcai.2019/509},
 year = {2019}
}

@inproceedings{kumar2012learning,
 author = {Abhishek Kumar and
Hal Daum{\'{e}} III},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/KumarD12.bib},
 booktitle = {Proceedings of the 29th International Conference on Machine Learning,
{ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
 publisher = {icml.cc / Omnipress},
 timestamp = {Wed, 03 Apr 2019 01:00:00 +0200},
 title = {Learning Task Grouping and Overlap in Multi-task Learning},
 url = {http://icml.cc/2012/papers/690.pdf},
 year = {2012}
}

@inproceedings{standley2020tasks,
 author = {Trevor Standley and
Amir Roshan Zamir and
Dawn Chen and
Leonidas J. Guibas and
Jitendra Malik and
Silvio Savarese},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/StandleyZCGMS20.bib},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
 pages = {9120--9132},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 15 Dec 2020 00:00:00 +0100},
 title = {Which Tasks Should Be Learned Together in Multi-task Learning?},
 url = {http://proceedings.mlr.press/v119/standley20a.html},
 volume = {119},
 year = {2020}
}

@article{fifty2021efficiently,
 author = {Fifty, Chris and Amid, Ehsan and Zhao, Zhe and Yu, Tianhe and Anil, Rohan and Finn, Chelsea},
 journal = {Advances in Neural Information Processing Systems},
 title = {Efficiently identifying task groupings for multi-task learning},
 volume = {34},
 year = {2021}
}

@inproceedings{xue2006semantic,
 address = {New York City, USA},
 author = {Xue, Nianwen},
 booktitle = {Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference},
 pages = {431--438},
 publisher = {Association for Computational Linguistics},
 title = {Semantic role labeling of nominalized predicates in {C}hinese},
 url = {https://aclanthology.org/N06-1055},
 year = {2006}
}

@inproceedings{mukherjee2020uncertainty,
 author = {Subhabrata Mukherjee and
Ahmed Hassan Awadallah},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/MukherjeeA20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Uncertainty-aware Self-training for Few-shot Text Classification},
 url = {https://proceedings.neurips.cc/paper/2020/hash/f23d125da1e29e34c552f448610ff25f-Abstract.html},
 year = {2020}
}

@article{wang2020adaptive,
 author = {Wang, Yaqing and Mukherjee, Subhabrata and Chu, Haoda and Tu, Yuancheng and Wu, Ming and Gao, Jing and Awadallah, Ahmed Hassan},
 journal = {ArXiv preprint},
 title = {Adaptive self-training for few-shot neural sequence labeling},
 url = {https://arxiv.org/abs/2010.03680},
 volume = {abs/2010.03680},
 year = {2020}
}

@inproceedings{xie2020self,
 author = {Qizhe Xie and
Minh{-}Thang Luong and
Eduard H. Hovy and
Quoc V. Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/XieLHL20.bib},
 booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
 doi = {10.1109/CVPR42600.2020.01070},
 pages = {10684--10695},
 publisher = {{IEEE}},
 timestamp = {Tue, 11 Aug 2020 01:00:00 +0200},
 title = {Self-Training With Noisy Student Improves ImageNet Classification},
 url = {https://doi.org/10.1109/CVPR42600.2020.01070},
 year = {2020}
}

@inproceedings{zoph2020rethinking,
 author = {Barret Zoph and
Golnaz Ghiasi and
Tsung{-}Yi Lin and
Yin Cui and
Hanxiao Liu and
Ekin Dogus Cubuk and
Quoc Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ZophGLCLC020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Rethinking Pre-training and Self-training},
 url = {https://proceedings.neurips.cc/paper/2020/hash/27e9661e033a73a6ad8cefcde965c54d-Abstract.html},
 year = {2020}
}

@inproceedings{he2019revisiting,
 author = {Junxian He and
Jiatao Gu and
Jiajun Shen and
Marc'Aurelio Ranzato},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HeGSR20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {Revisiting Self-Training for Neural Sequence Generation},
 url = {https://openreview.net/forum?id=SJgdnAVKDH},
 year = {2020}
}

@inproceedings{wang2020minilm,
 author = {Wenhui Wang and
Furu Wei and
Li Dong and
Hangbo Bao and
Nan Yang and
Ming Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/WangW0B0020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
of Pre-Trained Transformers},
 url = {https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2020}
}

@inproceedings{mukherjee2020xtremedistil,
 address = {Online},
 author = {Mukherjee, Subhabrata  and
Hassan Awadallah, Ahmed},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.202},
 pages = {2221--2234},
 publisher = {Association for Computational Linguistics},
 title = {{X}treme{D}istil: Multi-stage Distillation for Massive Multilingual Models},
 url = {https://aclanthology.org/2020.acl-main.202},
 year = {2020}
}

@inproceedings{blum1998combining,
 author = {Blum, Avrim and Mitchell, Tom},
 booktitle = {Proceedings of the eleventh annual conference on Computational learning theory},
 pages = {92--100},
 title = {Combining labeled and unlabeled data with co-training},
 year = {1998}
}

@inproceedings{wu2018reinforced,
 address = {New Orleans, Louisiana},
 author = {Wu, Jiawei  and
Li, Lei  and
Wang, William Yang},
 booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
 doi = {10.18653/v1/N18-1113},
 pages = {1252--1262},
 publisher = {Association for Computational Linguistics},
 title = {Reinforced Co-Training},
 url = {https://aclanthology.org/N18-1113},
 year = {2018}
}

@article{zhou2005tri,
 author = {Zhou, Zhi-Hua and Li, Ming},
 journal = {IEEE Transactions on knowledge and Data Engineering},
 number = {11},
 pages = {1529--1541},
 publisher = {IEEE},
 title = {Tri-training: Exploiting unlabeled data using three classifiers},
 volume = {17},
 year = {2005}
}

@article{caruana1997multitask,
 author = {Caruana, Rich},
 journal = {Machine learning},
 number = {1},
 pages = {41--75},
 publisher = {Springer},
 title = {Multitask learning},
 volume = {28},
 year = {1997}
}

@article{zhang2021survey,
 author = {Zhang, Yu and Yang, Qiang},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 publisher = {IEEE},
 title = {A survey on multi-task learning},
 year = {2021}
}

@inproceedings{liu2007semi,
 author = {Qiuhua Liu and
Xuejun Liao and
Lawrence Carin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LiuLC07.bib},
 booktitle = {Advances in Neural Information Processing Systems 20, Proceedings
of the Twenty-First Annual Conference on Neural Information Processing
Systems, Vancouver, British Columbia, Canada, December 3-6, 2007},
 editor = {John C. Platt and
Daphne Koller and
Yoram Singer and
Sam T. Roweis},
 pages = {937--944},
 publisher = {Curran Associates, Inc.},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Semi-Supervised Multitask Learning},
 url = {https://proceedings.neurips.cc/paper/2007/hash/a34bacf839b923770b2c360eefa26748-Abstract.html},
 year = {2007}
}

@inproceedings{li2009active,
 author = {Li, Hui and Liao, Xuejun and Carin, Lawrence},
 booktitle = {2009 IEEE International Conference on Acoustics, Speech and Signal Processing},
 organization = {IEEE},
 pages = {1637--1640},
 title = {Active learning for semi-supervised multi-task learning},
 year = {2009}
}

@article{mackay1992information,
 author = {MacKay, David JC},
 journal = {Neural computation},
 number = {4},
 pages = {590--604},
 publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
 title = {Information-based objective functions for active data selection},
 volume = {4},
 year = {1992}
}

@inproceedings{su2019improving,
 address = {Florence, Italy},
 author = {Su, Hui  and
Shen, Xiaoyu  and
Zhang, Rongzhi  and
Sun, Fei  and
Hu, Pengwei  and
Niu, Cheng  and
Zhou, Jie},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1003},
 pages = {22--31},
 publisher = {Association for Computational Linguistics},
 title = {Improving Multi-turn Dialogue Modelling with Utterance {R}e{W}riter},
 url = {https://aclanthology.org/P19-1003},
 year = {2019}
}

@inproceedings{pan2019improving,
 address = {Hong Kong, China},
 author = {Pan, Zhufeng  and
Bai, Kun  and
Wang, Yan  and
Zhou, Lianqiang  and
Liu, Xiaojiang},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1191},
 pages = {1824--1833},
 publisher = {Association for Computational Linguistics},
 title = {Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration},
 url = {https://aclanthology.org/D19-1191},
 year = {2019}
}

@inproceedings{elgohary2019can,
 address = {Hong Kong, China},
 author = {Elgohary, Ahmed  and
Peskov, Denis  and
Boyd-Graber, Jordan},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1605},
 pages = {5918--5924},
 publisher = {Association for Computational Linguistics},
 title = {Can You Unpack That? Learning to Rewrite Questions-in-Context},
 url = {https://aclanthology.org/D19-1605},
 year = {2019}
}

@inproceedings{huang2021sarg,
 author = {Huang, Mengzuo and Li, Feng and Zou, Wuhe and Zhang, Hongbo and Zhang, Weidong},
 booktitle = {Proc. 35th AAAI Conf. Artif. Intell., 33rd Conf. Innovat. Appl. Artif. Intell., 11th Symp. Educat. Adv. Artif. Intell.},
 pages = {13055--13063},
 title = {SARG: A novel semi autoregressive generator for multi-turn incomplete utterance restoration},
 year = {2021}
}

@inproceedings{hao2021rast,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Hao, Jie  and
Song, Linfeng  and
Wang, Liwei  and
Xu, Kun  and
Tu, Zhaopeng  and
Yu, Dong},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.402},
 pages = {4913--4924},
 publisher = {Association for Computational Linguistics},
 title = {{RAST}: Domain-Robust Dialogue Rewriting as Sequence Tagging},
 url = {https://aclanthology.org/2021.emnlp-main.402},
 year = {2021}
}

@article{jin2022hierarchical,
 author = {Jin, Lisa and Song, Linfeng and Jin, Lifeng and Yu, Dong and Gildea, Daniel},
 title = {Hierarchical Context Tagging for Utterance Rewriting},
 year = {2022}
}

@inproceedings{tarvainen2017mean,
 author = {Antti Tarvainen and
Harri Valpola},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/TarvainenV17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {1195--1204},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results},
 url = {https://proceedings.neurips.cc/paper/2017/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html},
 year = {2017}
}

@inproceedings{chen2021semi,
 author = {Chen, Xiaokang and Yuan, Yuhui and Zeng, Gang and Wang, Jingdong},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {2613--2622},
 title = {Semi-supervised semantic segmentation with cross pseudo supervision},
 year = {2021}
}

@article{xu2021conversational,
 author = {Xu, Kun and Wu, Han and Song, Linfeng and Zhang, Haisong and Song, Linqi and Yu, Dong},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 pages = {2465--2475},
 publisher = {IEEE},
 title = {Conversational semantic role labeling},
 volume = {29},
 year = {2021}
}

@inproceedings{wang2020large,
 author = {Wang, Yida and Ke, Pei and Zheng, Yinhe and Huang, Kaili and Jiang, Yong and Zhu, Xiaoyan and Huang, Minlie},
 booktitle = {CCF International Conference on Natural Language Processing and Chinese Computing},
 organization = {Springer},
 pages = {91--103},
 title = {A large-scale chinese short-text conversation dataset},
 year = {2020}
}

@inproceedings{wu2021csagn,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Wu, Han  and
Xu, Kun  and
Song, Linqi},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.177},
 pages = {2312--2317},
 publisher = {Association for Computational Linguistics},
 title = {{CSAGN}: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling},
 url = {https://aclanthology.org/2021.emnlp-main.177},
 year = {2021}
}

@inproceedings{bhat2021self,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Bhat, Meghana Moorthy  and
Sordoni, Alessandro  and
Mukherjee, Subhabrata},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.836},
 pages = {10702--10712},
 publisher = {Association for Computational Linguistics},
 title = {Self-training with Few-shot Rationalization},
 url = {https://aclanthology.org/2021.emnlp-main.836},
 year = {2021}
}

@inproceedings{yu2021self,
 address = {Punta Cana, Dominican Republic},
 author = {Yu, Dian  and
Sun, Kai  and
Yu, Dong  and
Cardie, Claire},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 doi = {10.18653/v1/2021.findings-emnlp.6},
 pages = {56--68},
 publisher = {Association for Computational Linguistics},
 title = {Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question-Answering Data},
 url = {https://aclanthology.org/2021.findings-emnlp.6},
 year = {2021}
}

@inproceedings{devlin2018bert,
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 url = {https://aclanthology.org/N19-1423},
 year = {2019}
}

@inproceedings{vaswani2017attention,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@inproceedings{loshchilov2017decoupled,
 author = {Ilya Loshchilov and
Frank Hutter},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Decoupled Weight Decay Regularization},
 url = {https://openreview.net/forum?id=Bkg6RiCqY7},
 year = {2019}
}

@inproceedings{morris2004and,
 author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
 booktitle = {Eighth International Conference on Spoken Language Processing},
 title = {From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition},
 year = {2004}
}

@inproceedings{lin2004rouge,
 address = {Barcelona, Spain},
 author = {Lin, Chin-Yew},
 booktitle = {Text Summarization Branches Out},
 pages = {74--81},
 publisher = {Association for Computational Linguistics},
 title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
 url = {https://aclanthology.org/W04-1013},
 year = {2004}
}

@inproceedings{he-choi-2021-stem,
 address = {Online and Punta Cana, Dominican Republic},
 author = {He, Han  and
Choi, Jinho D.},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.451},
 pages = {5555--5577},
 publisher = {Association for Computational Linguistics},
 title = {The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders},
 url = {https://aclanthology.org/2021.emnlp-main.451},
 year = {2021}
}

@inproceedings{du2020self,
 address = {Online},
 author = {Du, Jingfei  and
Grave, Edouard  and
Gunel, Beliz  and
Chaudhary, Vishrav  and
Celebi, Onur  and
Auli, Michael  and
Stoyanov, Veselin  and
Conneau, Alexis},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.426},
 pages = {5408--5418},
 publisher = {Association for Computational Linguistics},
 title = {Self-training Improves Pre-training for Natural Language Understanding},
 url = {https://aclanthology.org/2021.naacl-main.426},
 year = {2021}
}

@inproceedings{li2021task,
 address = {Punta Cana, Dominican Republic},
 author = {Li, Shiyang  and
Yavuz, Semih  and
Chen, Wenhu  and
Yan, Xifeng},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 doi = {10.18653/v1/2021.findings-emnlp.86},
 pages = {1006--1015},
 publisher = {Association for Computational Linguistics},
 title = {Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding},
 url = {https://aclanthology.org/2021.findings-emnlp.86},
 year = {2021}
}

@article{chen2022vlp,
  title={Vlp: A survey on vision-language pre-training},
  author={Chen, Feilong and Zhang, Duzhen and Han, Minglun and Chen, Xiuyi and Shi, Jing and Xu, Shuang and Xu, Bo},
  journal={arXiv preprint arXiv:2202.09061},
  year={2022}
}

@inproceedings{zhang-etal-2021-video,
    title = "Video-aided Unsupervised Grammar Induction",
    author = "Zhang, Songyang  and
      Song, Linfeng  and
      Jin, Lifeng  and
      Xu, Kun  and
      Yu, Dong  and
      Luo, Jiebo",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.119",
    doi = "10.18653/v1/2021.naacl-main.119",
    pages = "1513--1524",
    abstract = "We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on grammar induction from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction.",
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@ARTICLE{Ma2020-yj,
  title    = "Self-paced Multi-view Co-training",
  author   = "Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi",
  journal  = "J. Mach. Learn. Res.",
  volume   =  21,
  number   =  57,
  pages    = "1--38",
  year     =  2020,
  keywords = "co-train"
}

@INPROCEEDINGS{Blum1998-ay,
  title     = "Combining Labeled and Unlabeled Data with {Co-Training}",
  booktitle = "{COLT}",
  author    = "Blum, Avrim and Mitchell, Tom",
  pages     = "92--100",
  year      =  1998,
  keywords  = "co-train"
}

@MISC{Wang_undated-qa,
  title        = "A New Analysis of {Co-Training}",
  author       = "Wang, Wei",
  abstract     = "In this paper, we present a new analysis on co-training, a
                  representative paradigm of disagreement-based semi-supervised
                  learning methods. In our analysis the co-training process is
                  viewed as a combinative label propagation over two views;
                  this provides a possibility to bring the graph-based and
                  disagreementbased semi-supervised methods into a unified
                  framework. With the analysis we get some insight that has not
                  been disclosed by previous theoretical studies. In
                  particular, we provide the sufficient and necessary condition
                  for co-training to succeed. We also discuss the relationship
                  to previous theoretical results and give some other
                  interesting implications of our results, such as combination
                  of weight matrices and view split.",
  howpublished = "\url{https://icml.cc/Conferences/2010/papers/275.pdf}",
  note         = "Accessed: 2022-2-3",
  keywords     = "co-train"
}

@INPROCEEDINGS{Mihalcea2004-ks,
  title     = "Co-training and Self-training for Word Sense Disambiguation",
  booktitle = "Proceedings of the Eighth Conference on Computational Natural
               Language Learning ({{C}o{NLL}-2004}) at {HLT}-{NAACL} 2004",
  author    = "Mihalcea, Rada",
  publisher = "Association for Computational Linguistics",
  pages     = "33--40",
  year      =  2004,
  address   = "Boston, Massachusetts, USA",
  keywords  = "co-train"
}

@ARTICLE{Kaljahi_undated-ua,
  title    = "Investigation of Co-training Views and Variations for Semantic
              Role Labeling",
  author   = "Kaljahi, Rasoul Samad Zadeh and Baba, Mohd Sapiyan",
  keywords = "co-train"
}

@INPROCEEDINGS{Li2014-ie,
  title     = "Ambiguity-aware Ensemble Training for Semi-supervised Dependency
               Parsing",
  booktitle = "Proceedings of the 52nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  author    = "Li, Zhenghua and Zhang, Min and Chen, Wenliang",
  publisher = "Association for Computational Linguistics",
  pages     = "457--467",
  month     =  jun,
  year      =  2014,
  address   = "Baltimore, Maryland",
  keywords  = "anthology;co-train"
}

@INPROCEEDINGS{Mikolov2013-mr,
  title     = "Linguistic Regularities in Continuous Space Word Representations",
  booktitle = "Proceedings of the 2013 Conference of the North {A}merican
               Chapter of the Association for Computational Linguistics: Human
               Language Technologies",
  author    = "Mikolov, Tomas and Yih, Wen-Tau and Zweig, Geoffrey",
  publisher = "Association for Computational Linguistics",
  pages     = "746--751",
  month     =  jun,
  year      =  2013,
  address   = "Atlanta, Georgia",
  keywords  = "anthology;co-train"
}

@INPROCEEDINGS{McClosky2006-mp,
  title     = "Effective {Self-Training} for Parsing",
  booktitle = "Proceedings of the Human Language Technology Conference of the
               {{NAACL}}, Main Conference",
  author    = "McClosky, David and Charniak, Eugene and Johnson, Mark",
  publisher = "Association for Computational Linguistics",
  pages     = "152--159",
  month     =  jun,
  year      =  2006,
  address   = "New York City, USA",
  keywords  = "anthology;co-train"
}

@INPROCEEDINGS{Wang2007-lq,
  title     = "Analyzing Co-training Style Algorithms",
  booktitle = "Machine Learning: {ECML} 2007",
  author    = "Wang, Wei and Zhou, Zhi-Hua",
  abstract  = "Co-training is a semi-supervised learning paradigm which trains
               two learners respectively from two different views and lets the
               learners label some unlabeled examples for each other. In this
               paper, we present a new PAC analysis on co-training style
               algorithms. We show that the co-training process can succeed
               even without two views, given that the two learners have large
               difference, which explains the success of some co-training style
               algorithms that do not require two views. Moreover, we
               theoretically explain that why the co-training process could not
               improve the performance further after a number of rounds, and
               present a rough estimation on the appropriate round to terminate
               co-training to avoid some wasteful learning rounds.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "454--465",
  year      =  2007,
  keywords  = "co-train"
}

@INPROCEEDINGS{Wan2009-jq,
  title     = "{Co-Training} for {Cross-Lingual} Sentiment Classification",
  booktitle = "Proceedings of the Joint Conference of the 47th Annual Meeting
               of the {ACL} and the 4th International Joint Conference on
               Natural Language Processing of the {AFNLP}",
  author    = "Wan, Xiaojun",
  publisher = "Association for Computational Linguistics",
  pages     = "235--243",
  month     =  aug,
  year      =  2009,
  address   = "Suntec, Singapore",
  keywords  = "co-train"
}

@INPROCEEDINGS{Zhou2012-kg,
  title     = "{Self-Training} with {Selection-by-Rejection}",
  booktitle = "2012 {IEEE} 12th International Conference on Data Mining",
  author    = "Zhou, Yan and Kantarcioglu, Murat and Thuraisingham, Bhavani",
  abstract  = "Practical machine learning and data mining problems often face
               shortage of labeled training data. Self-training algorithms are
               among the earliest attempts of using unlabeled data to enhance
               learning. Traditional self-training algorithms label unlabeled
               data on which classifiers trained on limited training data have
               the highest confidence. In this paper, a self-training algorithm
               that decreases the disagreement region of hypotheses is
               presented. The algorithm supplements the training set with
               self-labeled instances. Only instances that greatly reduce the
               disagreement region of hypotheses are labeled and added to the
               training set. Empirical results demonstrate that the proposed
               self-training algorithm can effectively improve classification
               performance.",
  pages     = "795--803",
  month     =  dec,
  year      =  2012,
  keywords  = "Noise;Training;Algorithm design and analysis;Semisupervised
               learning;Distributed databases;Labeling;Accuracy;semi-supervised
               learning;self-training;co-train"
}

@MISC{Chen_undated-xe,
  title        = "{Co-Training} for Domain Adaptation",
  author       = "Chen, Minmin and Weinberger, Kilian Q and Blitzer, John C",
  abstract     = "Domain adaptation algorithms seek to generalize a model
                  trained in a source domain to a new target domain. In many
                  practical cases, the source and target distributions can
                  differ substantially, and in some cases crucial target
                  features may not have support in the source domain. In this
                  paper we introduce an algorithm that bridges the gap between
                  source and target domains by slowly adding to the training
                  set both the target features and instances in which the
                  current algorithm is the most confident. Our algorithm is a
                  variant of co-training [7], and we name it CODA (Co-training
                  for domain adaptation). Unlike the original co-training work,
                  we do not assume a particular feature split. Instead, for
                  each iteration of cotraining, we formulate a single
                  optimization problem which simultaneously learns a target
                  predictor, a split of the feature space into views, and a
                  subset of source and target features to include in the
                  predictor. CODA significantly out-performs the
                  state-of-the-art on the 12-domain benchmark data set of
                  Blitzer et al. [4]. Indeed, over a wide range (65 of 84
                  comparisons) of target supervision CODA achieves the best
                  performance.",
  howpublished = "\url{https://proceedings.neurips.cc/paper/2011/file/93fb9d4b16aa750c7475b6d601c35c2c-Paper.pdf}",
  note         = "Accessed: 2022-2-2",
  keywords     = "co-train"
}

@INPROCEEDINGS{Chen2021-md,
  title     = "Revisiting Self-training for Few-shot Learning of Language Model",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing",
  author    = "Chen, Yiming and Zhang, Yan and Zhang, Chen and Lee, Grandee and
               Cheng, Ran and Li, Haizhou",
  abstract  = "As unlabeled data carry rich task-relevant information, they are
               proven useful for few-shot learning of language model. The
               question is how to effectively make use of such data. In this
               work, we revisit the self-training technique for language model
               fine-tuning and present a state-of-the-art prompt-based few-shot
               learner, SFLM. Given two views of a text sample via weak and
               strong augmentation techniques, SFLM generates a pseudo label on
               the weakly augmented version. Then, the model predicts the same
               pseudo label when fine-tuned with the strongly augmented
               version. This simple approach is shown to outperform other
               state-of-the-art supervised and semi-supervised counterparts on
               six sentence classification and six sentence-pair classification
               benchmarking tasks. In addition, SFLM only relies on a few
               in-domain unlabeled data. We conduct a comprehensive analysis to
               demonstrate the robustness of our proposed approach under
               various settings, including augmentation techniques, model
               scale, and few-shot knowledge transfer across tasks.",
  publisher = "Association for Computational Linguistics",
  pages     = "9125--9135",
  month     =  nov,
  year      =  2021,
  address   = "Online and Punta Cana, Dominican Republic",
  keywords  = "co-train"
}

@INPROCEEDINGS{Lee2021-uj,
  title     = "Co-training for Commit Classification",
  booktitle = "Proceedings of the Seventh Workshop on Noisy User-generated Text
               ({W-NUT} 2021)",
  author    = "Lee, Jian Yi David and Chieu, Hai Leong",
  abstract  = "Commits in version control systems (e.g. Git) track changes in a
               software project. Commits comprise noisy user-generated natural
               language and code patches. Automatic commit classification (CC)
               has been used to determine the type of code maintenance
               activities performed, as well as to detect bug fixes in code
               repositories. Much prior work occurs in the fully-supervised
               setting -- a setting that can be a stretch in resource-scarce
               situations presenting difficulties in labeling commits. In this
               paper, we apply co-training, a semi-supervised learning method,
               to take advantage of the two views available -- the commit
               message (natural language) and the code changes (programming
               language) -- to improve commit classification.",
  publisher = "Association for Computational Linguistics",
  pages     = "389--395",
  month     =  nov,
  year      =  2021,
  address   = "Online",
  keywords  = "co-train"
}

@INPROCEEDINGS{Chen2018-as,
  title           = "Co-training embeddings of knowledge graphs and entity
                     descriptions for cross-lingual entity alignment",
  booktitle       = "Proceedings of the {Twenty-Seventh} International Joint
                     Conference on Artificial Intelligence",
  author          = "Chen, Muhao and Tian, Yingtao and Chang, Kai-Wei and
                     Skiena, Steven and Zaniolo, Carlo",
  abstract        = "Multilingual knowledge graph (KG) embeddings provide
                     latent semantic representations of entities and structured
                     knowledge with cross-lingual inferences, which benefit
                     various knowledge-driven cross-lingual NLP tasks. However,
                     precisely learning such cross-lingual inferences is
                     usually hindered by the low coverage of entity alignment
                     in many KGs. Since many multilingual KGs also provide
                     literal descriptions of entities, in this paper, we
                     introduce an embedding-based approach which leverages a
                     weakly aligned multilingual KG for semi-supervised
                     cross-lingual learning using entity descriptions. Our
                     approach performs co-training of two embedding models,
                     i.e. a multilingual KG embedding model and a multilingual
                     literal description embedding model. The models are
                     trained on a large Wikipedia-based trilingual dataset
                     where most entity alignment is unknown to training.
                     Experimental results show that the performance of the
                     proposed approach on the entity alignment task improves at
                     each iteration of co-training, and eventually reaches a
                     stage at which it significantly surpasses previous
                     approaches. We also show that our approach has promising
                     abilities for zero-shot entity alignment, and
                     cross-lingual KG completion.",
  publisher       = "International Joint Conferences on Artificial Intelligence
                     Organization",
  month           =  jul,
  year            =  2018,
  address         = "California",
  keywords        = "co-train",
  conference      = "Twenty-Seventh International Joint Conference on
                     Artificial Intelligence \{IJCAI-18\}",
  location        = "Stockholm, Sweden"
}

@INPROCEEDINGS{Liu2021-xu,
  title     = "Unsupervised Conversation Disentanglement through {Co-Training}",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing",
  author    = "Liu, Hui and Shi, Zhan and Zhu, Xiaodan",
  abstract  = "Conversation disentanglement aims to separate intermingled
               messages into detached sessions, which is a fundamental task in
               understanding multi-party conversations. Existing work on
               conversation disentanglement relies heavily upon human-annotated
               datasets, which is expensive to obtain in practice. In this
               work, we explore training a conversation disentanglement model
               without referencing any human annotations. Our method is built
               upon the deep co-training algorithm, which consists of two
               neural networks: a message-pair classifier and a session
               classifier. The former is responsible of retrieving local
               relations between two messages while the latter categorizes a
               message to a session by capturing context-aware information.
               Both the two networks are initialized respectively with pseudo
               data built from the unannotated corpus. During the deep
               co-training process, we use the session classifier as a
               reinforcement learning component to learn a session assigning
               policy by maximizing the local rewards given by the message-pair
               classifier. For the message-pair classifier, we enrich its
               training data by retrieving message pairs with high confidence
               from the disentangled sessions predicted by the session
               classifier. Experimental results on the large Movie Dialogue
               Dataset demonstrate that our proposed approach achieves
               competitive performance compared to previous supervised methods.
               Further experiments show that the predicted disentangled
               conversations can promote the performance on the downstream task
               of multi-party response selection.",
  publisher = "Association for Computational Linguistics",
  pages     = "2345--2356",
  month     =  nov,
  year      =  2021,
  address   = "Online and Punta Cana, Dominican Republic",
  keywords  = "co-train"
}

@INPROCEEDINGS{Li2008-ex,
  title     = "{{C}o{CQA}}: {{C}o-{T}raining} over Questions and Answers with
               an Application to Predicting Question Subjectivity Orientation",
  booktitle = "Proceedings of the 2008 Conference on Empirical Methods in
               Natural Language Processing",
  author    = "Li, Baoli and Liu, Yandong and Agichtein, Eugene",
  publisher = "Association for Computational Linguistics",
  pages     = "937--946",
  month     =  oct,
  year      =  2008,
  address   = "Honolulu, Hawaii",
  keywords  = "co-train"
}

@INPROCEEDINGS{Caragea2015-fm,
  title     = "{Co-Training} for Topic Classification of Scholarly Data",
  booktitle = "Proceedings of the 2015 Conference on Empirical Methods in
               Natural Language Processing",
  author    = "Caragea, Cornelia and Bulgarov, Florin and Mihalcea, Rada",
  publisher = "Association for Computational Linguistics",
  pages     = "2357--2366",
  month     =  sep,
  year      =  2015,
  address   = "Lisbon, Portugal",
  keywords  = "co-train"
}

@INPROCEEDINGS{Steedman2003-it,
  title     = "Bootstrapping statistical parsers from small datasets",
  booktitle = "10th Conference of the {E}uropean Chapter of the Association for
               Computational Linguistics",
  author    = "Steedman, Mark and Osborne, Miles and Sarkar, Anoop and Clark,
               Stephen and Hwa, Rebecca and Hockenmaier, Julia and Ruhlen, Paul
               and Baker, Steven and Crim, Jeremiah",
  publisher = "Association for Computational Linguistics",
  month     =  apr,
  year      =  2003,
  address   = "Budapest, Hungary",
  keywords  = "co-train"
}

@INPROCEEDINGS{Abney2002-qo,
  title     = "Bootstrapping",
  booktitle = "the 40th Annual Meeting of the Association for Computational
               Linguistics ({ACL})",
  author    = "Abney, Steven",
  pages     = "360--367",
  year      =  2002,
  keywords  = "co-train"
}

@ARTICLE{Collier2020-hf,
  title         = "Routing Networks with Co-training for Continual Learning",
  author        = "Collier, Mark and Kokiopoulou, Efi and Gesmundo, Andrea and
                   Berent, Jesse",
  abstract      = "The core challenge with continual learning is catastrophic
                   forgetting, the phenomenon that when neural networks are
                   trained on a sequence of tasks they rapidly forget
                   previously learned tasks. It has been observed that
                   catastrophic forgetting is most severe when tasks are
                   dissimilar to each other. We propose the use of sparse
                   routing networks for continual learning. For each input,
                   these network architectures activate a different path
                   through a network of experts. Routing networks have been
                   shown to learn to route similar tasks to overlapping sets of
                   experts and dissimilar tasks to disjoint sets of experts. In
                   the continual learning context this behaviour is desirable
                   as it minimizes interference between dissimilar tasks while
                   allowing positive transfer between related tasks. In
                   practice, we find it is necessary to develop a new training
                   method for routing networks, which we call co-training which
                   avoids poorly initialized experts when new tasks are
                   presented. When combined with a small episodic memory replay
                   buffer, sparse routing networks with co-training outperform
                   densely connected networks on the MNIST-Permutations and
                   MNIST-Rotations benchmarks.",
  month         =  sep,
  year          =  2020,
  keywords      = "co-train",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2009.04381"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Dasgupta_undated-mk,
  title        = "{PAC} generalization bounds for co-training",
  author       = "Dasgupta, Sanjoy and Littman, Michael L and Mcallester, David",
  abstract     = "The rule-based bootstrapping introduced by Yarowsky, and its
                  cotraining variant by Blum and Mitchell, have met with
                  considerable empirical success. Earlier work on the theory of
                  co-training has been only loosely related to empirically
                  useful co-training algorithms. Here we give a new PAC-style
                  bound on generalization error which justifies both the use of
                  confidences-partial rules and partial labeling of the
                  unlabeled data-and the use of an agreement-based objective
                  function as suggested by Collins and Singer. Our bounds apply
                  to the multiclass case, i.e., where instances are to be
                  assigned one of labels for \textcent{} \!\` ¤ \pounds{} .",
  howpublished = "\url{https://papers.nips.cc/paper/2001/file/4c144c47ecba6f8318128703ca9e2601-Paper.pdf}",
  note         = "Accessed: 2022-2-4",
  keywords     = "co-train"
}

@ARTICLE{Wei2020-wj,
  title         = "Theoretical Analysis of {Self-Training} with Deep Networks
                   on Unlabeled Data",
  author        = "Wei, Colin and Shen, Kendrick and Chen, Yining and Ma,
                   Tengyu",
  abstract      = "Self-training algorithms, which train a model to fit
                   pseudolabels predicted by another previously-learned model,
                   have been very successful for learning with unlabeled data
                   using neural networks. However, the current theoretical
                   understanding of self-training only applies to linear
                   models. This work provides a unified theoretical analysis of
                   self-training with deep networks for semi-supervised
                   learning, unsupervised domain adaptation, and unsupervised
                   learning. At the core of our analysis is a simple but
                   realistic ``expansion'' assumption, which states that a low
                   probability subset of the data must expand to a neighborhood
                   with large probability relative to the subset. We also
                   assume that neighborhoods of examples in different classes
                   have minimal overlap. We prove that under these assumptions,
                   the minimizers of population objectives based on
                   self-training and input-consistency regularization will
                   achieve high accuracy with respect to ground-truth labels.
                   By using off-the-shelf generalization bounds, we immediately
                   convert this result to sample complexity guarantees for
                   neural nets that are polynomial in the margin and
                   Lipschitzness. Our results help explain the empirical
                   successes of recently proposed self-training algorithms
                   which use input consistency regularization.",
  month         =  oct,
  year          =  2020,
  keywords      = "co-train",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2010.03622"
}

@ARTICLE{Zhou2005-om,
  title     = "Tri-training: exploiting unlabeled data using three classifiers",
  author    = "Zhou, Zhi-Hua and Li, Ming",
  journal   = "IEEE Trans. Knowl. Data Eng.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  17,
  number    =  11,
  pages     = "1529--1541",
  month     =  nov,
  year      =  2005,
  keywords  = "co-train"
}

@ARTICLE{Angluin1988-yw,
  title     = "Learning from noisy examples",
  author    = "Angluin, Dana and Laird, Philip",
  journal   = "Mach. Learn.",
  publisher = "Springer Nature",
  volume    =  2,
  number    =  4,
  pages     = "343--370",
  month     =  apr,
  year      =  1988,
  keywords  = "co-train",
  language  = "en"
}

@ARTICLE{Wagner2021-qo,
  title         = "Revisiting Tri-training of Dependency Parsers",
  author        = "Wagner, Joachim and Foster, Jennifer",
  abstract      = "We compare two orthogonal semi-supervised learning
                   techniques, namely tri-training and pretrained word
                   embeddings, in the task of dependency parsing. We explore
                   language-specific FastText and ELMo embeddings and
                   multilingual BERT embeddings. We focus on a low resource
                   scenario as semi-supervised learning can be expected to have
                   the most impact here. Based on treebank size and available
                   ELMo models, we select Hungarian, Uyghur (a zero-shot
                   language for mBERT) and Vietnamese. Furthermore, we include
                   English in a simulated low-resource setting. We find that
                   pretrained word embeddings make more effective use of
                   unlabelled data than tri-training but that the two
                   approaches can be successfully combined.",
  month         =  sep,
  year          =  2021,
  keywords      = "co-train",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.08122"
}


@inproceedings{su-etal-2019-improving,
    title = "Improving Multi-turn Dialogue Modelling with Utterance {R}e{W}riter",
    author = "Su, Hui  and
      Shen, Xiaoyu  and
      Zhang, Rongzhi  and
      Sun, Fei  and
      Hu, Pengwei  and
      Niu, Cheng  and
      Zhou, Jie",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    address = "Florence, Italy",
    pages = "22--31",
}


@inproceedings{pan-etal-2019-improving,
    title = "Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration",
    author = "Pan, Zhufeng  and
      Bai, Kun  and
      Wang, Yan  and
      Zhou, Lianqiang  and
      Liu, Xiaojiang",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    address = "Hong Kong, China",
    pages = "1824--1833",
}